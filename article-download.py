#!/usr/bin/env python3
import requests
from bs4 import BeautifulSoup
import csv
import re
import jaconv  # pip install jaconv
from datetime import datetime

# ç§»è¡Œå¯¾è±¡ã®URLãƒªã‚¹ãƒˆ
urls = [
    'https://www.rivage.jp/news/227.html',
    'https://www.rivage.jp/news/225.html',
    'https://www.rivage.jp/news/230.html',
    'https://www.rivage.jp/news/229.html',
    'https://www.rivage.jp/news/228.html',
    'https://www.rivage.jp/news/58.html',
    'https://www.rivage.jp/news/221.html',
    'https://www.rivage.jp/news/191.html',
    'https://www.rivage.jp/news/142.html',
    'https://www.rivage.jp/news/201.html',
    'https://www.rivage.jp/news/208.html',
    'https://www.rivage.jp/news/188.html',
    'https://www.rivage.jp/news/181.html',
    'https://www.rivage.jp/news/118.html',
    'https://www.rivage.jp/news/107.html',
]

with open('rivage_news.csv', 'w', newline='', encoding='utf-8') as f:
    writer = csv.DictWriter(f, fieldnames=[
        'post_title','post_name','post_date','post_status',
        'post_author','post_category','post_excerpt',
        'post_content','custom_field_original_id'
    ])
    writer.writeheader()

    for url in urls:
        # ãƒšãƒ¼ã‚¸ã‚’ãƒã‚¤ãƒˆã§å–å¾—
        try:
            res = requests.get(url)
            res.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"Error fetching {url}: {e}")
            continue

        # CP932 (Windows-31J) ã§ãƒ‘ãƒ¼ã‚¹ã—ã¦çµµæ–‡å­—ã‚’å«ã‚€å…¨ã¦ã®æ–‡å­—ã‚’å–å¾—
        soup = BeautifulSoup(res.content, 'html.parser', from_encoding='cp932')

        # ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã® <ul class="news">
        news_ul = soup.select_one('ul.news')
        if not news_ul:
            print(f"Skip {url}: <ul class=\"news\"> not found")
            continue

        # å­ li è¦ç´ ã‚’é †ç•ªã«å–å¾—
        items = news_ul.find_all('li', recursive=False)
        if not items:
            print(f"Skip {url}: news items not found")
            continue

        # â‘  æ—¥ä»˜ï¼‹ã‚¿ã‚¤ãƒˆãƒ«ã® li
        head_li = items[0]
        strings = list(head_li.stripped_strings)
        # ä¾‹: ['2025/04/15', 'ãŠçŸ¥ã‚‰ã›', '5/5 (æ—¥ãƒ»ç¥) ã“ã©ã‚‚ã®æ—¥ã‚±ãƒ¼äºˆç´„æ‰¿ã‚Šã¾ã™ğŸ']
        if len(strings) < 2:
            print(f"Skip {url}: cannot parse date/title")
            continue

        date_text  = strings[0]            # 'YYYY/MM/DD'
        title_text = strings[-1]           # æœ€å¾Œã®æ–‡å­—åˆ—ã‚’ã‚¿ã‚¤ãƒˆãƒ«ã¨ã™ã‚‹

        # æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆå¤±æ•—ã—ãŸã‚‰ä»Šæ—¥ã®æ—¥ä»˜ï¼‰
        try:
            dt = datetime.strptime(date_text, '%Y/%m/%d')
            post_date = dt.strftime('%Y-%m-%d 00:00:00')
        except ValueError:
            post_date = datetime.now().strftime('%Y-%m-%d 00:00:00')

        # ã‚¹ãƒ©ãƒƒã‚°ç”Ÿæˆ
        raw_slug = jaconv.kana2alphabet(title_text)
        slug = re.sub(r'[^a-z0-9]+', '-', raw_slug.lower()).strip('-')

        # â‘¡ æœ¬æ–‡ li.newsTxt
        content_li = next((li for li in items if 'newsTxt' in (li.get('class') or [])), None)
        # æŠœç²‹
        excerpt = ''
        if content_li:
            excerpt_tag = content_li.select_one('p')
            excerpt = excerpt_tag.get_text(strip=True) if excerpt_tag else ''
        # æœ¬æ–‡HTML
        content_html = str(content_li) if content_li else ''

        # å…ƒã‚µã‚¤ãƒˆID
        m = re.search(r'/news/(\d+)\.html$', url)
        original_id = m.group(1) if m else ''

        writer.writerow({
            'post_title': title_text,
            'post_name': slug,
            'post_date': post_date,
            'post_status': 'publish',
            'post_author': 'admin',
            'post_category': 'news',
            'post_excerpt': excerpt,
            'post_content': content_html,
            'custom_field_original_id': original_id,
        })

print("CSVå‡ºåŠ›ãŒå®Œäº†ã—ã¾ã—ãŸ: rivage_news.csv")
